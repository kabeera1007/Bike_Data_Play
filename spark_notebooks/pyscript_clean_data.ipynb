{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8f1ee-2aff-43de-9377-63f0ea5f04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import year, month, date_format, to_timestamp, dayofweek, hour\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, sum, desc, mean, stddev, min, max, avg, when, count\n",
    "from pyspark.sql.functions import radians, sin, cos, atan2, sqrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb886b-f959-4db2-8664-bc30258aa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859623d-1eb1-4426-85b2-36b7af023dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/raw/divvy/*', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ebd4b-8a0b-48e9-a655-40d76ffff5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new schema\n",
    "new_schema = StructType([\n",
    "    StructField(\"ride_id\", StringType(), True),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"started_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"start_lat\", FloatType(), True),\n",
    "    StructField(\"start_lng\", FloatType(), True),\n",
    "    StructField(\"end_lat\", FloatType(), True),\n",
    "    StructField(\"end_lng\", FloatType(), True),\n",
    "    StructField(\"member_casual\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9c76d-5991-492e-acd1-a7618089bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with the defined schema\n",
    "df = spark.read.schema(new_schema).csv('data/raw/divvy/*', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f0d2b-2a4d-4187-8587-b09cbe247e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Imputer\n",
    "imputer = Imputer(inputCols=['end_lat', 'end_lng'], outputCols=['end_lat_imputed', 'end_lng_imputed'])\n",
    "# Fit the Imputer model\n",
    "imputer_model = imputer.fit(df)\n",
    "# Transform the data\n",
    "df = imputer_model.transform(df)\n",
    "df = df.drop('end_lat', 'end_lng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420405b0-ba0c-4b73-b9ca-0554caf5e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    (col(\"start_lat\") != 0) & (col(\"start_lng\") != 0) & (col(\"end_lat\") != 0) & (col(\"end_lng\") != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c0315-0a55-4899-bb6f-6f9289d0b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"ride_duration\",\n",
    "    (unix_timestamp(\"ended_at\") - unix_timestamp(\"started_at\")) / 60  # Duration in minutes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60552d3f-48e8-473b-a838-14cd98428749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"time_of_day\",\n",
    "    hour(col(\"started_at\")).alias(\"hour\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c846dd3-5fbc-41cf-b657-b66c34e38c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'started_at' to timestamp if not already\n",
    "df = df.withColumn(\"started_at\", col(\"started_at\").cast(\"timestamp\"))\n",
    "\n",
    "# Create the 'day_of_week' column (1 = Sunday, 7 = Saturday in Spark)\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"started_at\")))\n",
    "\n",
    "df = df.withColumn(\"day_of_week\",\n",
    "                                       when(col(\"day_of_week\") == 1, \"Sunday\")\n",
    "                                       .when(col(\"day_of_week\") == 2, \"Monday\")\n",
    "                                       .when(col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "                                       .when(col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "                                       .when(col(\"day_of_week\") == 5, \"Thursday\")\n",
    "                                       .when(col(\"day_of_week\") == 6, \"Friday\")\n",
    "                                       .when(col(\"day_of_week\") == 7, \"Saturday\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8815642-bc4c-4135-b9ee-184c3d7bf8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'start_station_name' or 'end_station_id' have null values\n",
    "df = df.dropna(subset=[\"start_station_name\", \"end_station_id\" , \"start_station_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262267a4-64ff-4d50-bd24-35a48f5f91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add partition columns (adjust the column name as needed)\n",
    "df = df.withColumn('year', year('started_at'))  # Use 'started_at' as the timestamp column\n",
    "df = df.withColumn('month', month('started_at'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79169197-9e00-4bdf-bfd7-2b2cab7e62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_repartitioned = df.repartition(\"year\", \"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6ad16-7d47-4b6f-8e5b-261bc7e5d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet with compression (Snappy) and partition by 'year' and 'month'\n",
    "data_repartitioned.write.option(\"compression\", \"snappy\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"pyscript_clean.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e538475-74dc-4983-bdc0-a9fd56e9400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
